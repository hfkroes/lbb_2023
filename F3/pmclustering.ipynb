{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540303b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hecto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hecto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "SEED = 2\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a20e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_data(data):\n",
    "    pmids = []\n",
    "    pyears = []\n",
    "    journals = []\n",
    "    authors = []\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    arttypes = []\n",
    "    langs = []\n",
    "    \n",
    "    entries = data.split('\\n\\n')\n",
    "\n",
    "    for entry in entries:\n",
    "        pmid = ''\n",
    "        pyear = ''\n",
    "        journal = ''\n",
    "        authors = ''\n",
    "        title = ''\n",
    "        abstract = ''\n",
    "        arttype = ''\n",
    "        lang = ''\n",
    "        \n",
    "        lines = entry.split('\\n')\n",
    "        abstract_started = False\n",
    "\n",
    "        for line in lines:\n",
    "            if re.match(r'^PMID-\\s', line):\n",
    "                pmid = line[re.search(r'-(.*)', line).start() + 1:].strip()\n",
    "            elif re.match(r'^DP\\s\\s-\\s', line):\n",
    "                year_match = re.search(r'\\d{4}', line)\n",
    "                if year_match:\n",
    "                    pyear = year_match.group()\n",
    "            elif re.match(r'^JT\\s\\s-\\s', line):\n",
    "                journal = line[re.search(r'-(.*)', line).start() + 1:].strip()\n",
    "            elif re.match(r'^TI\\s\\s-\\s', line):\n",
    "                title = line[re.search(r'-(.*)', line).start() + 1:].strip()\n",
    "            elif re.match(r'^PT\\s\\s-\\s', line):\n",
    "                arttype = line[re.search(r'-(.*)', line).start() + 1:].strip()\n",
    "            elif re.match(r'^LA\\s\\s-\\s', line):\n",
    "                lang = line[re.search(r'-(.*)', line).start() + 1:].strip()\n",
    "            else:\n",
    "                if re.match(r'^AB\\s\\s-\\s', line):\n",
    "                    abstract_started = True\n",
    "                    abstract += line[re.search(r'-(.*)', line).start() + 1:].strip()\n",
    "                elif not re.match(r'^\\s\\s\\s\\s\\s\\s', line):\n",
    "                    if abstract_started:\n",
    "                        break\n",
    "                elif abstract_started:\n",
    "                    abstract += ' ' + line.strip()\n",
    "\n",
    "        titles.append(title)\n",
    "        \n",
    "        if pmid != '':\n",
    "            pmids.append(pmid)\n",
    "        else:\n",
    "            pmids.append('NA')\n",
    "        \n",
    "        if pyear != '':\n",
    "            pyears.append(int(pyear))\n",
    "        else:\n",
    "            pyears.append('NA')\n",
    "            \n",
    "        if journal != '':\n",
    "            journals.append(journal)\n",
    "        else:\n",
    "            journals.append('NA')\n",
    "            \n",
    "        if lang != '':\n",
    "            langs.append(lang)\n",
    "        else:\n",
    "            langs.append('NA')\n",
    "\n",
    "        if abstract != '':\n",
    "            abstracts.append(abstract)\n",
    "        else:\n",
    "            abstracts.append('NA')\n",
    "\n",
    "    return pmids, titles, pyears, abstracts, arttypes, langs, journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351dde4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PMID- 33735179\\nOWN - NLM\\nSTAT- MEDLINE\\nDCOM- 20210726\\nLR  - 20210726\\nIS  - 1545-7885 (Electronic)\\nIS  - 1544-9173 (Print)\\nIS  - 1544-9173 (Linking)\\nVI  - 19\\nIP  - 3\\nDP  - 2021 Mar\\nTI  - Is \"bioinformatics\" dead?\\nPG  - e3001165\\nLID - 10.1371/journal.pbio.3001165 [doi]\\nLID - e3001165\\nAB  - Why would a computational biologist with 40 years of research experience say \\n      bioinformatics is dead? The short answer is, in being the Founding Dean of a new \\n      School of Data Science, what we do sudd'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'set3.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21a3ae9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Title</th>\n",
       "      <th>YearPub</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33735179</td>\n",
       "      <td>Is \"bioinformatics\" dead?</td>\n",
       "      <td>2021</td>\n",
       "      <td>Why would a computational biologist with 40 ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24348234</td>\n",
       "      <td>Education in computational biology today and t...</td>\n",
       "      <td>2013</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28899250</td>\n",
       "      <td>Again, What Is Nursing Science?</td>\n",
       "      <td>2017</td>\n",
       "      <td>This article again asks, What is nursing scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23012581</td>\n",
       "      <td>Perspectives on an education in computational ...</td>\n",
       "      <td>2012</td>\n",
       "      <td>The mainstream application of massively parall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33228539</td>\n",
       "      <td>Microbiology research at the systems biology a...</td>\n",
       "      <td>2020</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PMID                                              Title  YearPub  \\\n",
       "0  33735179                          Is \"bioinformatics\" dead?     2021   \n",
       "1  24348234  Education in computational biology today and t...     2013   \n",
       "2  28899250                    Again, What Is Nursing Science?     2017   \n",
       "3  23012581  Perspectives on an education in computational ...     2012   \n",
       "4  33228539  Microbiology research at the systems biology a...     2020   \n",
       "\n",
       "                                            Abstract  \n",
       "0  Why would a computational biologist with 40 ye...  \n",
       "1                                                 NA  \n",
       "2  This article again asks, What is nursing scien...  \n",
       "3  The mainstream application of massively parall...  \n",
       "4                                                 NA  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmids, titles, pyears, abstracts, arttypes, langs, journals = extract_article_data(data)\n",
    "\n",
    "na_column = ['NA' for x in range(len(pmids))\n",
    "\n",
    "article_data = {'PMID': pmids, 'YearPub':pyears, 'Journal': journals, 'Authors': na_column,\n",
    "                'Title': titles, 'Abstract': abstracts, 'articleType': arttypes, 'language': langs,\n",
    "               'pmcCitationCount'}\n",
    "\n",
    "article_df = pd.DataFrame(article_data)\n",
    "\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d666c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = article_df[article_df['Abstract'] != \"NA\"]\n",
    "corpus = article_df['Abstract'].values.astype('U')\n",
    "corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "custom_stopwords = set(stopwords.words(\"english\")+ [\"study\", \"group\", \"patient\", \"used\", \n",
    "                                                     \"disease\", \"effect\", \"method\", \"also\",\n",
    "                                                     \"result\", \"two\", \"may\",\"level\",\n",
    "                                                     \"participant\",\"treatment\",\"associated\"\n",
    "                                                    \"risk\", \"however\",\"year\",\n",
    "                                                     \"the\", \"this\",\"using\", \"showed\", \"analysis\",\"text\",\n",
    "                                                    \"abstract\",\"figure\", \"article\", \"genomics\",\n",
    "                                                   \"student\", \"research\", \"genetic\", \"bioinformatics\",\n",
    "                                                   \"bioinformatic\", \"health\", \"biology\", \"science\",\n",
    "                                                   \"genomic\", \"data\", \"education\", \"genetics\", \n",
    "                                                   \"gene\", \"genome\", \"nursing\", \"knowledge\", \"information\"])\n",
    "\n",
    "custom_stopwords = [Lemmatizer.lemmatize(word) for word in custom_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(corpus):\n",
    "    abstracts =[]\n",
    "    for line in corpus:\n",
    "        line.replace(\"\\n\", \"\")\n",
    "        line = line.lower()\n",
    "        line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "        line = re.sub('[^A-Za-z]', ' ', line)\n",
    "        new = ' '\n",
    "        for word in line.split():\n",
    "            word = Lemmatizer.lemmatize(word)\n",
    "            if word not in custom_stopwords and len(word)>3:\n",
    "                new = new + ' ' + word\n",
    "        abstracts.append(new)\n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92835f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_list = clean_text(corpus)\n",
    "\n",
    "abstract_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69319eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(abst_list):\n",
    "    abstract_tokens =[]\n",
    "    for line in abst_list:\n",
    "        tokens = word_tokenize(line)\n",
    "        tokens = [t for t in tokens if len(t) > 3]\n",
    "        abstract_tokens.append(tokens)\n",
    "    return abstract_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tokens = tokenizer(abstract_list)\n",
    "\n",
    "model = Word2Vec(sentences=abstract_tokens, workers=1, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e331a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "  features = []\n",
    "\n",
    "  for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "  return features\n",
    "\n",
    "vectorized_docs = vectorize(abstract_tokens, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_clusters = list(range(2,11))\n",
    "clusters = []\n",
    "n_cluster = []\n",
    "inertia_vals = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "  cluster_model = KMeans(n_clusters=n_clusters, random_state=5)\n",
    "  cluster_model.fit(vectorized_docs)\n",
    "\n",
    "  clusters.append(cluster_model)\n",
    "  inertia_vals.append(cluster_model.inertia_)\n",
    "  n_cluster.append(n_clusters)\n",
    "\n",
    "range_n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,10,1),inertia_vals,marker='*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd439b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,9,1):\n",
    "  print(\"----------------------\")\n",
    "  print('cluster',n_cluster[i])\n",
    "  print(clusters[i])\n",
    "  print(\"Silhouette score: \", silhouette_score(vectorized_docs,clusters[i].predict(vectorized_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed87910",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "\n",
    "k_means = KMeans(n_clusters=k, random_state=5)\n",
    "k_means.fit(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c831fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=5)\n",
    "\n",
    "reduced_vectors = pca.fit_transform(vectorized_docs)\n",
    "reduced_clusters = pca.fit_transform(k_means.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5096c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(reduced_vectors[:,0],reduced_vectors[:,1],c=k_means.labels_)\n",
    "plt.scatter(reduced_clusters[:, 0],reduced_clusters[:, 1], marker='x', s=150, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df['preprocessed_abstract'] = abstract_list\n",
    "article_df['labels'] = k_means.labels_\n",
    "article_df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordcloud(text):\n",
    "    word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a84f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for clus in range (2):\n",
    "    text =' '\n",
    "    df2 = article_df.loc[article_df[\"labels\"]==clus]\n",
    "    \n",
    "    for abst in df2.preprocessed_abstract:\n",
    "        text += abst\n",
    "    get_wordcloud(text)\n",
    "    \n",
    "    words = []\n",
    "    for i, j in get_top_n_words(df2[\"preprocessed_abstract\"],10):\n",
    "        words.append(i)\n",
    "    print (\"Top 10 words from cluster\",clus,\":\")\n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e676f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df[article_df['labels']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033bb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df[article_df['labels']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a41c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df['Journal'] = pd.Series([1 for x in range(len(df.index))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
